{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWh_eJdIddPq"
      },
      "source": [
        "### import and implement model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCRtcKOQddPs"
      },
      "outputs": [],
      "source": [
        "# packages\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Conv2d, LeakyReLU, MaxPool2d, Linear # import them seperetly because I think its more readable\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "import cv2\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO1WRLUyIEel"
      },
      "source": [
        "device settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Nb3z1SiYddPt"
      },
      "outputs": [],
      "source": [
        "# set seed\n",
        "torch.manual_seed(126)\n",
        "\n",
        "# potential opti\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# cores set\n",
        "torch.set_num_threads(8)\n",
        "torch.set_num_interop_threads(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avamSIV5IEeo"
      },
      "source": [
        "local settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "7zSIiJlvIEeo",
        "outputId": "47802de1-5642-4e90-adb2-7e4ce2b69656"
      },
      "outputs": [],
      "source": [
        "os.chdir('C:/Users/dalto/OneDrive/Pictures/Documents/Projects/Fracture/')\n",
        "class_dir = './images/class_ids.csv'\n",
        "image_dir = './images/resize_data'\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY5yl9wiIEeq"
      },
      "source": [
        "colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGeVYpTvddPv",
        "outputId": "b961d14f-d167-49b9-819c-1124298604aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# directories for classes and images\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "class_dir = '/content/drive/MyDrive/colab/class_ids.csv'\n",
        "image_dir = '/content/drive/Othercomputers/My Laptop/resize_data'\n",
        "batch_size = 128 # batch size, up from 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RekxyVUIEes"
      },
      "source": [
        "image dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjAPeYh_IEet"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, class_dir, img_dir): \n",
        "        self.img_labels = pd.read_csv(class_dir)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224), antialias=True), # resize because some of the images did not correctly resize locallhy\n",
        "                transforms.ConvertImageDtype(torch.float32), # Convert to float\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_filename = self.img_labels.iloc[idx, 0]\n",
        "        img_path = os.path.join(self.img_dir, img_filename)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) # use CV2 because native torch cant read BMP files. \n",
        "        # BMP files were used because they train much more efficently\n",
        "        image = np.expand_dims(image, axis=0) # convert to correct demensions (1, w, h)\n",
        "        image = torch.from_numpy(image).float().div(255.0) # convert to tensor\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert label to tensor (assuming classification)\n",
        "        label = torch.tensor(label, dtype=torch.int8)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39KNd9FpIEet"
      },
      "outputs": [],
      "source": [
        "# load dataset using made class function\n",
        "data_set = ImageDataset(class_dir, image_dir) # create dataset\n",
        "\n",
        "# set train and test set\n",
        "train_size = int(.75 * len(data_set)) # somewaht arb \n",
        "test_size = int(.15 * len(data_set))\n",
        "val_size = len(data_set) - test_size - train_size\n",
        "\n",
        "# random split\n",
        "training, testing, val = random_split(data_set, [train_size, test_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwrREV61ddPw"
      },
      "outputs": [],
      "source": [
        "# load the split data on the tensors \n",
        "train_loader = DataLoader(training, batch_size=batch_size, num_workers=8, pin_memory=True,  prefetch_factor=4, shuffle=True) \n",
        "# workers to preload and increase training speed, pin and prfetch with the same goal, would lower if to train locally\n",
        "test_loader = DataLoader(testing, batch_size=batch_size, num_workers=8, pin_memory=True,  prefetch_factor=4,shuffle=True)\n",
        "val_loader = DataLoader(val, batch_size=batch_size, num_workers=8,pin_memory=True, prefetch_factor=4,shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86poE9r0ddPx"
      },
      "source": [
        "model without transfer learning (will add just wanted to build one from stratch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE5Wj9pdddPy"
      },
      "outputs": [],
      "source": [
        "# I chose to use a CNN for the image classifcation.\n",
        "# CNNs preform much better then feed forward networks for image classification tasks and are still easy to implement\n",
        "\n",
        "class CNN (nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 1 input layer, to 64 filters, stride of one pixel, 3x3 kernal, padding = (kernal - 1)/2\n",
        "        # 3 layers like this\n",
        "        self.conv1 = Conv2d(in_channels=1, out_channels=64, stride=1, kernel_size=3, padding=1)\n",
        "        self.Lrelu1 = LeakyReLU() # better preformance on average compared to regular ReLu\n",
        "        self.bn1 = nn.BatchNorm2d(64) # prevent exploding / vanishing gradients, had this problem early\n",
        "        self.conv2 = Conv2d(in_channels=64, out_channels=64, stride=1, kernel_size=5, padding=2)\n",
        "        self.Lrelu2 = LeakyReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.maxpool1 = MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "        # 3 layers like this\n",
        "        self.conv3 = Conv2d(in_channels=64, out_channels=128, stride=1, kernel_size=5, padding=2)\n",
        "        self.Lrelu3 = LeakyReLU() # better preformance on average compared to regular ReLu\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = Conv2d(in_channels=128, out_channels=128, stride=1, kernel_size=7, padding=3)\n",
        "        self.Lrelu4 = LeakyReLU()\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.maxpool2 = MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "        # 3 layers like this\n",
        "        self.conv5 = Conv2d(in_channels=128, out_channels=256, stride=1, kernel_size=5, padding=2)\n",
        "        self.Lrelu5 = LeakyReLU() # better preformance on average compared to regular ReLu\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.conv6 = Conv2d(in_channels=256, out_channels=256, stride=1, kernel_size=3, padding=1)\n",
        "        self.Lrelu6 = LeakyReLU()\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.maxpool3 = MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "        # reduce the number of features\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # fully connected layers\n",
        "        self.fc1 = Linear(256, 512) # pool layer reduces\n",
        "        self.relu1 = LeakyReLU()\n",
        "        self.fc3 = Linear(512, 256)\n",
        "        self.relu3 = LeakyReLU()\n",
        "        self.dropout2 = nn.Dropout(p = 0.4)\n",
        "        self.fc4 = nn.Linear(256, 1) # one output\n",
        "\n",
        "\n",
        "        # this reduces overfitting making one neuron not resposnible for everything, also improves regualrization\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through Convolutional Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.Lrelu1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.Lrelu2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        # Pass through Convolutional Block 2\n",
        "        x = self.conv3(x)\n",
        "        x = self.Lrelu3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.Lrelu4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        # Pass through Convolutional Block 3\n",
        "        x = self.conv5(x)\n",
        "        x = self.Lrelu5(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.Lrelu6(x)\n",
        "        x = self.bn6(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        # pooling layer\n",
        "        x = self.global_avg_pool(x)\n",
        "\n",
        "        # flatten\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Pass through Fully Connected Layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.dropout2(x) # Apply dropout\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        # Pass through the final Linear layer\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        # Apply Dropout, sigmoind applied in loss function, better preformance\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgPXYlfeddPz"
      },
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsRxUn7KddP0",
        "outputId": "7ba44378-ea0e-46ba-f409-34887f4f91c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available: True\n",
            "Number of CUDA devices: 1\n",
            "Device 0: NVIDIA L4\n"
          ]
        }
      ],
      "source": [
        "# check if cuda device is aval, multiple gpus on local device ensure which one\n",
        "cuda_available = torch.cuda.is_available()\n",
        "print(f\"CUDA Available: {cuda_available}\")\n",
        "if cuda_available:\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "\n",
        "device = \"cuda\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPsC6mgtddP0",
        "outputId": "7666cd34-21d9-444e-aa4b-10a9859dc948"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (Lrelu1): LeakyReLU(negative_slope=0.01)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (Lrelu2): LeakyReLU(negative_slope=0.01)\n",
              "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (Lrelu3): LeakyReLU(negative_slope=0.01)\n",
              "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "  (Lrelu4): LeakyReLU(negative_slope=0.01)\n",
              "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv5): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (Lrelu5): LeakyReLU(negative_slope=0.01)\n",
              "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (Lrelu6): LeakyReLU(negative_slope=0.01)\n",
              "  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "  (relu1): LeakyReLU(negative_slope=0.01)\n",
              "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (relu3): LeakyReLU(negative_slope=0.01)\n",
              "  (dropout2): Dropout(p=0.4, inplace=False)\n",
              "  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run model on GPU\n",
        "model = CNN()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdS1X4L2ddP1"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1 # start with high LR because of LR scheduler\n",
        "loss_fn = nn.BCEWithLogitsLoss() # add activation function in here\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate) # adam w has better preformance, weight decay is applied sep,\n",
        "# leads to more peak ram may have to reduce batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AQ_d86HddP1"
      },
      "outputs": [],
      "source": [
        "def val_set_test():\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    n_rounds = 0\n",
        "    with torch.no_grad():  # Disable gradient computation, no need for val set\n",
        "        for batch_idx, (image, label) in enumerate(val_loader):\n",
        "            # set up\n",
        "            image = image.to(device).float()\n",
        "            label = label.to(device).float()\n",
        "\n",
        "            # make predictions on val\n",
        "            predictions = model(image)\n",
        "            predictions = predictions.squeeze()\n",
        "            loss = loss_fn(predictions, label)\n",
        "\n",
        "            # loss\n",
        "            val_loss += loss.item()\n",
        "            n_rounds = batch_idx + 1\n",
        "\n",
        "    model.train()  # Set model back to training mode\n",
        "    return val_loss / n_rounds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQIx6ms72Ot0"
      },
      "outputs": [],
      "source": [
        "milestones = [8, 16, 30] # change LR, model converges pretty quickly in testing\n",
        "gamma = 0.1 # Multiply LR by 0.1 at each milestone\n",
        "scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klcOTFtzddP1"
      },
      "outputs": [],
      "source": [
        "def training(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    training_losses = []  # To track loss history\n",
        "    min_loss = float('inf') # es min loss\n",
        "    patience = 0 # es track\n",
        "    for i in range(epochs):\n",
        "        tloss = 0.0\n",
        "        n_rounds = 0\n",
        "        for batch_idx, (image, label) in enumerate(train_loader):\n",
        "          # Move data to device and ensure correct data types\n",
        "          image = image.to(device).float()\n",
        "          label = label.to(device).float()\n",
        "\n",
        "          # Zero gradients, so they do not accumlate \n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          predictions = model(image)\n",
        "          predictions = predictions.squeeze()\n",
        "          loss = loss_fn(predictions, label)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Accumulate loss\n",
        "          tloss += loss.item()\n",
        "          n_rounds = batch_idx + 1\n",
        "          avg_loss = tloss / n_rounds\n",
        "\n",
        "          # Update training loss stats\n",
        "          training_losses.append(avg_loss)\n",
        "\n",
        "        # early stopping\n",
        "        val_loss = val_set_test() # return loss for validation set\n",
        "\n",
        "        if val_loss > min_loss:\n",
        "          patience += 1\n",
        "\n",
        "        if val_loss < min_loss:\n",
        "          min_loss = val_loss\n",
        "          patience = 0\n",
        "\n",
        "        if patience > 6: # early stopping after 5 rounds, \n",
        "          #because of model arch and lack of training data the model started to converge v quickly and overtrain quick. \n",
        "          # could be fixed with simpler arch or more dropout, but simpler arch started to miss important features of image\n",
        "          # more dropout will be used if refiend later\n",
        "            print(f\"early stopping at round {i}\")\n",
        "            return model, training_losses\n",
        "        \n",
        "        # val process\n",
        "        print(f\"batch {batch_idx} is done\")\n",
        "\n",
        "        scheduler.step() # step for learning rate decay\n",
        "        print(f\"{avg_loss} is the average loss at epoch {i}\") # still provide avg and epoch after through early stopping\n",
        "\n",
        "\n",
        "    return model, training_losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to20MjqAddP1"
      },
      "source": [
        "profiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_A5NdTzddP2",
        "outputId": "afca5254-92ec-4c4f-ff4b-cf45c35043d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                          training_loop         0.00%       0.000us         0.00%       0.000us       0.000us       6.518ms       111.38%       6.518ms       6.518ms             1  \n",
            "                                          training_loop        17.05%       1.370ms        75.72%       6.085ms       6.085ms       0.000us         0.00%       5.852ms       5.852ms             1  \n",
            "                                           aten::conv2d         0.66%      52.985us        30.70%       2.467ms     411.192us       0.000us         0.00%       5.200ms     866.739us             6  \n",
            "                                      aten::convolution         1.37%     110.252us        30.04%       2.414ms     402.362us       0.000us         0.00%       5.200ms     866.739us             6  \n",
            "                                     aten::_convolution         1.76%     141.434us        28.67%       2.304ms     383.986us       0.000us         0.00%       5.200ms     866.739us             6  \n",
            "                                aten::cudnn_convolution        14.09%       1.132ms        24.40%       1.961ms     326.851us       4.992ms        85.32%       4.992ms     832.073us             6  \n",
            "_5x_cudnn_volta_scudnn_128x32_sliced1x4_ldg4_relu_ex...         0.00%       0.000us         0.00%       0.000us       0.000us       2.694ms        46.05%       2.694ms       1.347ms             2  \n",
            "        _5x_cudnn_volta_scudnn_128x32_relu_medium_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us       1.298ms        22.18%       1.298ms     648.962us             2  \n",
            "_5x_cudnn_volta_scudnn_128x64_relu_xregs_large_nn_v1...         0.00%       0.000us         0.00%       0.000us       0.000us     586.387us        10.02%     586.387us     586.387us             1  \n",
            "                                       aten::batch_norm         0.28%      22.441us        10.00%     803.837us     133.973us       0.000us         0.00%     296.827us      49.471us             6  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 8.037ms\n",
            "Self CUDA time total: 5.852ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get a batch of data from test_loader\n",
        "# Get only 2 batches (small subset) from test_loader\n",
        "images, _ = next(iter(train_loader))\n",
        "images = images[:1].to(device).float()  # Take only 2 samples, model was having issues running due to data pipeline issues,\n",
        "# used the small sample so I could look at the profiler and understand and fix\n",
        "\n",
        "with torch.profiler.profile(\n",
        "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "    record_shapes=True,\n",
        "    ) as prof:\n",
        "        # Code to be profiled, e.g., model inference or training loop\n",
        "        with torch.profiler.record_function(\"training_loop\"):\n",
        "            output = model(images)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ruxqm0UCddP2"
      },
      "source": [
        "#### training call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p86cBUzl62TL"
      },
      "outputs": [],
      "source": [
        "# empty ram and unused to prevent oom errors\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "A2D5SoSmddP2"
      },
      "outputs": [],
      "source": [
        "# model training loop, will stop before 100 epochs cause of early stopping\n",
        "t_model, t_loss = training(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwSWciCD5oXW"
      },
      "source": [
        "#### Model Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "UZL-wUBgeDFw"
      },
      "outputs": [],
      "source": [
        "save_dir = '/content/drive/MyDrive/colab/'\n",
        "PATH = os.path.join(save_dir, 'my_model_epoch_1.pth')\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqaZCK-BIEe1"
      },
      "source": [
        "#### Model Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Hf_BrHJmIEe2",
        "outputId": "553f67e5-e347-4901-e07c-66256ebefe61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (Lrelu1): LeakyReLU(negative_slope=0.01)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (Lrelu2): LeakyReLU(negative_slope=0.01)\n",
              "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (Lrelu3): LeakyReLU(negative_slope=0.01)\n",
              "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv4): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "  (Lrelu4): LeakyReLU(negative_slope=0.01)\n",
              "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv5): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (Lrelu5): LeakyReLU(negative_slope=0.01)\n",
              "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (Lrelu6): LeakyReLU(negative_slope=0.01)\n",
              "  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "  (relu1): LeakyReLU(negative_slope=0.01)\n",
              "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (relu3): LeakyReLU(negative_slope=0.01)\n",
              "  (dropout2): Dropout(p=0.4, inplace=False)\n",
              "  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/colab/my_model_epoch_1.pth'))\n",
        "model.eval() # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7nPrU5VIEe3"
      },
      "outputs": [],
      "source": [
        "# very simple testing of just accuray, in an actual proudction model would wanna focus on precison and recall and f1 score as well\n",
        "def evaluate_model_acc(model, test_loader, device):\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on the test set: {accuracy:.2f} %')\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJqOnGK-IEe4"
      },
      "outputs": [],
      "source": [
        "evaluate_model_acc(model, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
